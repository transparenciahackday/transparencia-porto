#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
import urllib
from BeautifulSoup import BeautifulSoup
#from BeautifulSoup import BeautifulStoneSoup
from datetime import datetime as dt
from json import dumps
from utils import *
#import sys


DEFAULT_MAX = 4230

ROMAN_NUMERALS = {
    'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5,
    'VI': 6, 'VII': 7, 'VIII': 8, 'IX': 9, 'X': 10,
    'XI': 11, 'XII': 12, 'XIII': 13, 'XIV': 14, 'XV': 15,
    'XVI': 16, 'XVII': 17, 'XVIII': 18, 'XIX': 19, 'XX': 20,
    'XXI': 21, 'XXII': 22, 'XXIII': 23, 'XXIV': 24, 'XXV': 25,
    }

DATASETS = '../../datasets/'
URL_DEPS_ACTIVOS='http://www.parlamento.pt/DeputadoGP/Paginas/DeputadosemFuncoes.aspx'
FORMATTER_URL_BIO_DEP='http://www.parlamento.pt/DeputadoGP/Paginas/Biografia.aspx?BID=%d'

def parse_legislature(s):
    s = s.replace('&nbsp;', '')
    number, dates = s.split('[')
    number = ROMAN_NUMERALS[number.strip()]
    dates = dates.strip(' ]')
    if len(dates.split(' a ')) == 2:
        start, end = dates.split(' a ')
    else:
        start = dates.split(' a ')[0]
        end = ''
    return number, start, end

def scrape(start=0, end=None, verbose=False, outfile='', indent=1):
    if not end:
        try:
            deps_activos_list = getpage(URL_DEPS_ACTIVOS)
            soup = BeautifulSoup(deps_activos_list)
        except: # hÃ¡ muitos erros http ou parse que podem ocorrer
            soup = None
            print 'Active MP page could not be fetched. Using a max ID value of %d.' % DEFAULT_MAX 
        
        if soup:
            max = 0
            table_deps = soup.find('table', 'ARTabResultados')
            deps = table_deps.findAll('tr', 'ARTabResultadosLinhaPar')
            deps += table_deps.findAll('tr', 'ARTabResultadosLinhaImpar')
            for dep in deps:
                depurl = dep.td.a['href']
                dep_bid = int(depurl[depurl.find('BID=')+4:])
                if dep_bid > max:
                    max = dep_bid
            max = int(max * 1.05)
        else:
            max = int(DEFAULT_MAX * 1.05)
        
        print 'Testing up to %d' % max
    else:
        max = end
    
    deprows = {}
    
    for i in range(start, max):
        if verbose: print i
        url = FORMATTER_URL_BIO_DEP % i
        soup = BeautifulSoup(getpage(url))
        name = soup.find('span',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_ucNome_rptContent_ctl01_lblText'))
        short = soup.find('span',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_lblNomeDeputado'))
        birthdate = soup.find('span',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_ucDOB_rptContent_ctl01_lblText'))
        party = soup.find('span',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_lblPartido'))
        profession = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlProf'))
        literacy = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlHabilitacoes'))
        jobs = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlCargosExercidos')) # ;)
        coms = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlComissoes'))
        legs = soup.find('table',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_gvTabLegs'))
        if name:
            deprows[i]= {'id': i,
                         'name': name.text,
                         'url': url,
                         'scrape_date': dt.utcnow().isoformat()}
            if short:
                deprows[i]['short'] = short.text
            if birthdate:
                deprows[i]['birthdate'] = birthdate.text
            if party:
                deprows[i]['party'] = party.text
            if profession:
                #TODO: break professions string into multiple entries, ';' is the separator
                #TODO: these blocks are repeated and should be made into functions
                deprows[i]['profession'] = []
                for each in profession.findAll('tr')[1:]:
                    deprows[i]['profession'].append(each.text)
            if literacy:
                deprows[i]['literacy'] = []
                for each in literacy.findAll('tr')[1:]:
                    deprows[i]['literacy'].append(each.text)
            if jobs:
                deprows[i]['jobs'] = []
                for each in jobs.findAll('tr')[1:]:
                    deprows[i]['jobs'].append(each.text)
            if coms:
                deprows[i]['comissions'] = []
                for each in coms.findAll('tr')[1:]:
                    deprows[i]['comissions'].append(each.text)
            if legs:
                deprows[i]['mandates'] = []
                for each in legs.findAll('tr')[1:]:
                    leg = each.findAll('td')
                    l = leg[0].text
                    number, start, end = parse_legislature(l)

                    deprows[i]['mandates'].append({'legislature': number, 'start_date': start, 'end_date': end, 'constituency': leg[3].text, 'party': leg[4].text})
            print name
    
    if outfile:
        depsfp = open(outfile, 'w+')
        depsfp.write(dumps(deprows, encoding='utf-8', indent=indent, sort_keys=True))
        depsfp.close()
    else:
        print dumps(deprows, encoding='utf-8', indent=indent, sort_keys=True)


if __name__ == '__main__':
    import sys
    import optparse
    parser = optparse.OptionParser()
    # TODO: Add an option to overwrite cache, in order to get up-to-date records
    parser.add_option('-s', '--start', 
                      dest="start", 
                      default="0",
                      help='Begin parsing from this ID (int required)'
                      )
    parser.add_option('-e', '--end', 
                      dest="end", 
                      default="",
                      help='Stop parsing on this ID (int required)'
                      )
    parser.add_option('-v', '--verbose',
                      dest="verbose",
                      default=False,
                      action="store_true",
                      help='Print verbose information',
                      )
    parser.add_option('-o', '--outfile', 
                      dest="outfile", 
                      default="",
                      help='Output JSON to this file'
                      )
    parser.add_option('-i', '--indent', 
                      dest="indent", 
                      default="1",
                      help='Number of spaces for indentation (default is 1)'
                      )
    '''
    parser.add_option('-p', '--picky',
                      dest="picky",
                      default=False,
                      action="store_true",
                      help='Stop batch processing in case an error is found',
                      )
    parser.add_option('-f', '--force',
                      dest="force",
                      default=False,
                      action="store_true",
                      help='Process file even if the output file already exists',
                      )
    '''
    options, remainder = parser.parse_args()
    start = int(options.start)
    end = int(options.end)
    verbose = options.verbose
    outfile = options.outfile
    indent = options.indent

    scrape(start, end, verbose, outfile)



