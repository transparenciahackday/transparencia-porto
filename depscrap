#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
depscrap
========

Descarrega a página de um deputado do Parlamento.pt e extrai uma parte da informação, convertendo para JSON.
Também obtém a lista de deputados em funções caso não seja indicado um intervalo de ID's.

Como usar
---------

Gravar o resultado num ficheiro:
    python depscrap -o deputados.json

Gravar o resultado num ficheiro com uma indentação de 4 espaços:
    python depscrap -o deputados.json -i 4

Para mostrar o resultado na linha de comandos:
    python depscrap

Para ver todas as opções possíveis:
    python depscrap -h

Ver também
----------
* interessesscrap.py
* pic_scrapper.py

"""
import urllib
from BeautifulSoup import BeautifulSoup
from datetime import datetime as dt
from json import dumps
from utils import *
from replaces_depscrap import SHORTNAME_REPLACES
import click
from zenlog import log


fieldnames = ['id', 'shortname', 'name', 'party', 'education', 'birthdate', 'occupation', 'current_jobs',
'jobs', 'commissions', 'mandates', 'awards', 'url', 'scrape_date']

DEFAULT_MAX = 5000

ROMAN_NUMERALS = {
    'I': 1, 'II': 2, 'III': 3, 'IV': 4, 'V': 5,
    'VI': 6, 'VII': 7, 'VIII': 8, 'IX': 9, 'X': 10,
    'XI': 11, 'XII': 12, 'XIII': 13, 'XIV': 14, 'XV': 15,
    'XVI': 16, 'XVII': 17, 'XVIII': 18, 'XIX': 19, 'XX': 20,
    'XXI': 21, 'XXII': 22, 'XXIII': 23, 'XXIV': 24, 'XXV': 25,
    }

DATASETS = '../thd-datasets/'
URL_DEPS_ACTIVOS='http://www.parlamento.pt/DeputadoGP/Paginas/DeputadosemFuncoes.aspx'
FORMATTER_URL_BIO_DEP='http://www.parlamento.pt/DeputadoGP/Paginas/Biografia.aspx?BID=%d'

def parse_legislature(s):
    s = s.replace('&nbsp;', '')
    number, dates = s.split('[')
    number = ROMAN_NUMERALS[number.strip()]
    dates = dates.strip(' ]')
    if len(dates.split(' a ')) == 2:
        start, end = dates.split(' a ')
    else:
        start = dates.split(' a ')[0]
        end = ''
    if start.endswith(' a'):
        start = start.replace(' a', '')
    return number, start, end

def scrape(format, start=1, end=None, verbose=False, outfile='', indent=1):
    if not end:
        try:
            deps_activos_list = getpage(URL_DEPS_ACTIVOS)
            soup = BeautifulSoup(deps_activos_list)
        except: # há muitos erros http ou parse que podem ocorrer
            soup = None
            log.warning( 'Active MP page could not be fetched. Using a max ID value of %d.' % DEFAULT_MAX)

        if soup:
            max = 0
            table_deps = soup.find('table', 'ARTabResultados')
            deps = table_deps.findAll('tr', 'ARTabResultadosLinhaPar')
            deps += table_deps.findAll('tr', 'ARTabResultadosLinhaImpar')
            for dep in deps:
                depurl = dep.td.a['href']
                dep_bid = int(depurl[depurl.find('BID=')+4:])
                if dep_bid > max:
                    max = dep_bid
            max = int(max * 1.05)
        else:
            max = int(DEFAULT_MAX * 1.05)

        log.info('Scraping until ID %d.' % max)
    else:
        max = end

    deprows = {}

    for i in range(start, max):
        if verbose:
            log.debug("Trying ID %d..." % i)
        url = FORMATTER_URL_BIO_DEP % i
        soup = BeautifulSoup(getpage(url))
        name = soup.find('span',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_ucNome_rptContent_ctl01_lblText'))
        short = soup.find('span',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_lblNomeDeputado'))
        birthdate = soup.find('span',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_ucDOB_rptContent_ctl01_lblText'))
        party = soup.find('span',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_lblPartido'))
        occupation = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlProf'))
        education = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlHabilitacoes'))
        current_jobs = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlCargosDesempenha')) # ;)
        jobs = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlCargosExercidos')) # ;)
        awards = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlCondecoracoes'))
        coms = soup.find('div',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_pnlComissoes'))
        mandates = soup.find('table',dict(id='ctl00_ctl13_g_8035397e_bdf3_4dc3_b9fb_8732bb699c12_ctl00_gvTabLegs'))
        if name:
            deprows[i]= {'id': i,
                         'name': name.text,
                         'url': url,
                         'scrape_date': dt.utcnow().isoformat()}
            if short:
                # replace by canonical shortnames if appropriate
                if short.text in SHORTNAME_REPLACES:
                    t = SHORTNAME_REPLACES[short.text]
                else:
                    t = short.text
                deprows[i]['shortname'] = t
            if birthdate:
                deprows[i]['birthdate'] = birthdate.text
            if party:
                deprows[i]['party'] = party.text
            if education:
                #TODO: break educations string into multiple entries, ';' is the separator
                #TODO: these blocks are repeated and should be made into functions
                deprows[i]['education'] = []
                for each in education.findAll('tr')[1:]:
                    text = each.find('span').text
                    deprows[i]['education'].append(text)
            if occupation:
                deprows[i]['occupation'] = []
                for each in occupation.findAll('tr')[1:]:
                    deprows[i]['occupation'].append(each.text)
            if jobs:
                deprows[i]['jobs'] = []
                for each in jobs.findAll('tr')[1:]:
                    if '\n' in each.text:
                        for j in each.text.split('\n'):
                            if j:
                                deprows[i]['jobs'].append(j.rstrip(' .;'))
                    else:
                        deprows[i]['jobs'].append(each.text)
            if current_jobs:
                deprows[i]['current_jobs'] = []
                for each in current_jobs.findAll('tr')[1:]:
                    if '\n' in each.text:
                        for j in each.text.split('\n'):
                            if j:
                                deprows[i]['current_jobs'].append(j.rstrip(' .;'))
                    else:
                        deprows[i]['current_jobs'].append(each.text.rstrip(' ;.'))
            if coms:
                deprows[i]['commissions'] = []
                for each in coms.findAll('tr')[1:]:
                    deprows[i]['commissions'].append(each.text)
            if awards:
                deprows[i]['awards'] = []
                for each in awards.findAll('tr')[1:]:
                    if '\n' in each.text:
                        for j in each.text.split('\n'):
                            if j:
                                deprows[i]['awards'].append(j.rstrip(' .;'))
                    else:
                        deprows[i]['awards'].append(each.text.rstrip(' ;.'))
            if mandates:
                deprows[i]['mandates'] = []
                for each in mandates.findAll('tr')[1:]:
                    leg = each.findAll('td')
                    l = leg[0].text
                    number, start, end = parse_legislature(l)

                    deprows[i]['mandates'].append({'legislature': number, 'start_date': start, 'end_date': end, 'constituency': leg[3].text, 'party': leg[4].text})

            if verbose:
                log.info("Found MP: %s" % short.text)

    log.info("Saving to file %s..." % outfile)
    depsfp = open(outfile, 'w+')
    if format == "json":
        depsfp.write(dumps(deprows, encoding='utf-8', indent=indent, sort_keys=True))
        depsfp.close()
    elif format == "csv":
        import unicodecsv as csv
        writer = csv.DictWriter(depsfp, delimiter=",", quoting=csv.QUOTE_NONNUMERIC, quotechar='"', fieldnames=fieldnames)
        writer.writeheader()
        for rownumber in deprows:
            row = deprows[rownumber]
            row.pop("mandates")
            for key in row:
                if type(row[key]) == list:
                    # convert lists to ;-separated strings
                    row[key] = "; ".join(row[key])
            writer.writerow(row)


# TODO: Add an option to overwrite cache, in order to get up-to-date records
@click.command()
@click.option("-f", "--format", help="Output file format, can be json (default) or csv", default="json")
@click.option("-s", "--start", type=int, help="Begin scrape from this ID (int required, default 0)", default=0)
@click.option("-e", "--end", type=int, help="End scrape at this ID (int required, default 5000)", default=5000)
@click.option("-v", "--verbose", is_flag=True, help="Print some helpful information when running")
@click.option("-o", "--outfile", type=click.Path(), help="Output file (default is deputados.json)")
@click.option("-i", "--indent", type=int, help="Spaces for JSON indentation (default is 2)", default=2)
def main(format, start, end, verbose, outfile, indent):
    if not outfile and format == "csv":
        outfile = "deputados.csv"
    elif not outfile and format == "json":
        outfile = "deputados.json"
    scrape(format, start, end, verbose, outfile, indent)

if __name__ == "__main__":
    main()

